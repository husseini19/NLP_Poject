{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"Copy of 3-Text-Corpus-and-Embeddings.ipynb","provenance":[{"file_id":"https://github.com/mdda/deep-learning-workshop/blob/master/notebooks/5-RNN/3-Text-Corpus-and-Embeddings.ipynb","timestamp":1587176457057}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"98nT5xCUKw23","colab_type":"text"},"source":["# Text Corpus and Embeddings\n","\n","This example trains a RNN to tag words from a corpus - \n","\n","The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"]},{"cell_type":"code","metadata":{"id":"Xi6ujUdOKw26","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","import os\n","import pickle\n","import time\n","\n","SENTENCE_LENGTH_MAX = 32\n","EMBEDDING_DIM=50"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"luxP8mUuKw2-","colab_type":"text"},"source":["## Basic Text and Parsing Tools"]},{"cell_type":"code","metadata":{"id":"brJNFPsPKw2_","colab_type":"code","outputId":"6a7cdb65-dc73-4837-9745-a07fb6ada6ba","executionInfo":{"status":"ok","timestamp":1587176482884,"user_tz":240,"elapsed":2373,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"HVgRMedrKw3E","colab_type":"code","outputId":"388c9336-21ef-460c-c825-516d14b1fb81","executionInfo":{"status":"ok","timestamp":1587176482885,"user_tz":240,"elapsed":2370,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n","sentence_splitter.tokenize(\"This is Mr. Smith's tokenized test. The U.S.A gives us sent two. Is this sent three?\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"This is Mr. Smith's tokenized test.\",\n"," 'The U.S.A gives us sent two.',\n"," 'Is this sent three?']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"UouXj36cKw3K","colab_type":"code","outputId":"cc20472f-88bf-44a0-8d5a-349f0e048530","executionInfo":{"status":"ok","timestamp":1587176482885,"user_tz":240,"elapsed":2366,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","tokenizer.tokenize(\"This is Mr. Smith's tokenized test.\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This', 'is', 'Mr.', 'Smith', \"'s\", 'tokenized', 'test', '.']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"omMrXSoGKw3N","colab_type":"text"},"source":["### Download a Wikipedia Corpus\n","\n","From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n","\n","Here's the paper that explains how the corpus was constructed : \n","\n","*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n","    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"]},{"cell_type":"code","metadata":{"id":"ftufhhx6Kw3N","colab_type":"code","colab":{}},"source":["corpus_dir = './data/RNN/'\n","corpus_text_file = os.path.join(corpus_dir, 'en.wikipedia.2010.100K.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PikMjeeRKw3Q","colab_type":"code","outputId":"17e3c1ad-32e6-4eba-bde2-7c96892dbc5a","executionInfo":{"status":"ok","timestamp":1587176486283,"user_tz":240,"elapsed":5755,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if not os.path.isfile( corpus_text_file ):\n","    if not os.path.exists(corpus_dir):\n","        os.makedirs(corpus_dir)\n","\n","    corpus_text_tar = 'eng_wikipedia_2010_100K.tar.gz'    \n","    download_url = 'http://pcai056.informatik.uni-leipzig.de/downloads/corpora/'+corpus_text_tar\n","\n","    data_cache = './data/cache'\n","    if not os.path.exists(data_cache):\n","        os.makedirs(data_cache)\n","    \n","    # Fall-back url if too slow\n","    #download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+corpus_text_tar\n","\n","    import shutil, requests\n","\n","    # Get the download path from the web-service\n","    #urllib.request.urlretrieve('http://wortschatz.uni-leipzig.de/download/service', corpus_text_tar)\n","    # download_url = ...\n","    \n","    tarfilepath = os.path.join(data_cache, corpus_text_tar)\n","    if not os.path.isfile( tarfilepath ):\n","        response = requests.get(download_url, stream=True)\n","        with open(tarfilepath, 'wb') as out_file:\n","            shutil.copyfileobj(response.raw, out_file)\n","    if os.path.isfile(tarfilepath):\n","        import tarfile\n","        #tarfile.open(tarfilepath, 'r:gz').extractall(corpus_dir)\n","        tarfile.open(tarfilepath, 'r:gz').extract('eng_wikipedia_2010_100K-sentences.txt', corpus_dir)\n","    shutil.move(os.path.join(corpus_dir, 'eng_wikipedia_2010_100K-sentences.txt'), corpus_text_file)\n","    \n","    # Get rid of tarfile source (the required text file itself will remain)\n","    #os.unlink(tarfilepath)\n","\n","print(\"Corpus available locally\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Corpus available locally\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KS026dOkKw3T","colab_type":"code","colab":{}},"source":["## This is a work-in-progress, since we should really discover 'download_url' from the 'service'\n","#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data='file=%s&func=\"link\"' % (corpus_text_tar,))\n","#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data=dict(file=corpus_text_tar, func=\"link\") )\n","#r.text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTUN9vBqKw3X","colab_type":"code","colab":{}},"source":["def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n","    while True:\n","        with open(corpus_text_file, encoding='utf-8') as f:\n","            for line in f.readlines():\n","                n,l = line.split('\\t')   # Strip of the initial numbers\n","                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n","                    tree_banked = tokenizer.tokenize(s)\n","                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n","                        yield tree_banked\n","        print(\"Corpus : Looping\")\n","corpus_sentence_tokens_gen = corpus_sentence_tokens()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qz9w-ZUKw3e","colab_type":"code","outputId":"4c10a4f8-549d-4694-8115-6de22cf0f8f3","executionInfo":{"status":"ok","timestamp":1587176486287,"user_tz":240,"elapsed":5746,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["' | '.join(next(corpus_sentence_tokens_gen))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Showing | that | even | in | the | modern | warfare | of | the | 1930s | and | 1940s | , | the | dilapidated | fortifications | still | had | defensive | usefulness | .'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"c9xJQ331Kw3g","colab_type":"text"},"source":["## GloVe Word Embeddings\n","Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/"]},{"cell_type":"code","metadata":{"id":"FleAKaQpKw3h","colab_type":"code","outputId":"10fe62f8-1266-4a96-f533-39381638cfa6","executionInfo":{"status":"ok","timestamp":1587176501305,"user_tz":240,"elapsed":20759,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"source":["! pip install glove_python"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting glove_python\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n","\r\u001b[K     |█▎                              | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n","Building wheels for collected packages: glove-python\n","  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700257 sha256=e3a2a578ed53a22987990412f81cdd324467e11209ca0558d65d04bf3a5a0c9e\n","  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n","Successfully built glove-python\n","Installing collected packages: glove-python\n","Successfully installed glove-python-0.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S8p7cElNKw3j","colab_type":"text"},"source":["### Create the Co-occurrence Matrix\n","For speed, this looks at the first 100,000 tokens in the corpus - and should create the co-occurences in 30 seconds or so."]},{"cell_type":"code","metadata":{"id":"_h9_d-tTKw3k","colab_type":"code","outputId":"7d9a0a3e-4fe0-44db-addb-50f91d808b59","executionInfo":{"status":"ok","timestamp":1587176524450,"user_tz":240,"elapsed":43899,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["import glove\n","glove_corpus = glove.Corpus()\n","\n","corpus_sentences = [ \n","        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n","        for _ in range(0,100*1000) \n","    ]\n","\n","# Fit the co-occurrence matrix using a sliding window of 10 words.\n","t0 = time.time()\n","glove_corpus.fit(corpus_sentences, window=10)\n","\n","print(\"Dictionary length=%d\" % (len(glove_corpus.dictionary),))\n","print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Corpus : Looping\n","Dictionary length=98815\n","Co-occurrence calculated in   5.7sec\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S1ecUx9OKw3p","colab_type":"code","outputId":"8749752e-5916-40e0-d88a-defd39ee3449","executionInfo":{"status":"ok","timestamp":1587176524450,"user_tz":240,"elapsed":43892,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Return the index of the word in the dictionary\n","glove_corpus.dictionary['city']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["544"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"6Sj8EKnIKw3u","colab_type":"text"},"source":["###  Create the Word Embedding\n","\n","This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."]},{"cell_type":"code","metadata":{"id":"pvM2pbQlKw3u","colab_type":"code","outputId":"763d0bab-1612-482f-a73e-1b5402c109d3","executionInfo":{"status":"ok","timestamp":1587176597408,"user_tz":240,"elapsed":116843,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":407}},"source":["word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n","\n","t0 = time.time()\n","glove_epochs, glove_threads = 20, 4 \n","\n","word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n","\n","print(\"%d-d word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n","        EMBEDDING_DIM, (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n","\n","# Add the word -> id dictionary to the model to allow similarity queries.\n","word_embedding.add_dictionary(glove_corpus.dictionary)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing 20 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","50-d word-embedding created in  73.0sec =  14.6sec per epoch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D9vEIov0Kw3w","colab_type":"code","colab":{}},"source":["#word_embedding.save(\"./data/RNN/glove.embedding.50.pkl\")\n","#word_embedding.load(\"./data/RNN/glove.embedding.50.pkl\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w0fLNTimKw30","colab_type":"text"},"source":["###  Test Word Embedding\n"]},{"cell_type":"code","metadata":{"id":"sEmTd3TbKw31","colab_type":"code","outputId":"0de5febb-df75-4873-c4c6-ea0692b19d20","executionInfo":{"status":"ok","timestamp":1587176597411,"user_tz":240,"elapsed":116840,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# word-similarity test\n","word_embedding.most_similar('country')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('property', 0.9530555653867256),\n"," ('division', 0.9502128974958951),\n"," ('title', 0.9491549923319751),\n"," ('house', 0.9459608792356191)]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"yA0aM7XiKw33","colab_type":"code","colab":{}},"source":["# word-analogy test\n","def get_embedding_vec(word):\n","    idx = word_embedding.dictionary.get(word.lower(), -1)\n","    if idx<0:\n","        #print(\"Missing word : '%s'\" % (word,))\n","        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n","    return word_embedding.word_vectors[idx]\n","\n","def get_closest_word(vec, number=5):\n","    dst = (np.dot(word_embedding.word_vectors, vec)\n","                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n","                   / np.linalg.norm(vec))\n","    word_ids = np.argsort(-dst)\n","    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n","            if x in word_embedding.inverse_dictionary]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpAmsMRVKw39","colab_type":"code","outputId":"851fb164-3138-4455-af2f-49946541f551","executionInfo":{"status":"ok","timestamp":1587176597587,"user_tz":240,"elapsed":117009,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["analogy_vec = get_embedding_vec('woman') + get_embedding_vec('king') - get_embedding_vec('man')\n","get_closest_word(analogy_vec)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('captain', 0.9431481405592956),\n"," ('king', 0.9370102809834094),\n"," ('prince', 0.935661368115408),\n"," ('emperor', 0.9223270105202688),\n"," ('boss', 0.9191233657819952)]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"oIRjydHjKw4A","colab_type":"code","colab":{}},"source":["def test_analogy(s='one two three four'):\n","    (a,b,c,d) = s.split(' ')\n","    analogy_vec = get_embedding_vec(b) - get_embedding_vec(a) + get_embedding_vec(c)\n","    words = [ w for (w,p) in get_closest_word(analogy_vec) if w not in (a,b,c)]\n","    print(\"'%s' is to '%s' as '%s' is to {%s}\" % (a,b,c,', '.join(words)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuBGRh1qKw4C","colab_type":"code","outputId":"48d3cb29-3802-49c7-cf34-3fafb69fc93f","executionInfo":{"status":"ok","timestamp":1587176597822,"user_tz":240,"elapsed":117235,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["test_analogy('man woman king queen')\n","test_analogy('paris france rome italy')\n","test_analogy('kitten cat puppy dog')\n","test_analogy('understand understood run ran')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'man' is to 'woman' as 'king' is to {captain, prince, emperor, boss}\n","'paris' is to 'france' as 'rome' is to {canada, russia, germany, japan}\n","'kitten' is to 'cat' as 'puppy' is to {cruiser, robot, riot, kitchen}\n","'understand' is to 'understood' as 'run' is to {being, another, wher, chosen, given}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bIBzUGuAKw4E","colab_type":"text"},"source":["### Problem : Embedding is *Poor*\n","### Solution : Load a pre-trained word embedding\n","\n","Since the embedding we learnt above is poor, let's load a pre-trained word embedding, from a much larger corpus, trained for a much longer period.  Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/ \n","\n","NB: If you don't have the required data, and the RedCatLabs server doesn't give you the download, the loader below downloads a 823Mb file via a fairly slow connection to a server at Stanford (this can take HOURS)."]},{"cell_type":"code","metadata":{"id":"YtH31n43Kw4E","colab_type":"code","outputId":"fce43afd-c27f-4763-9e7d-2c02ba8c1953","executionInfo":{"status":"ok","timestamp":1587176599716,"user_tz":240,"elapsed":119123,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import os, requests, shutil\n","\n","glove_dir = './data/RNN/'\n","glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n","glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n","\n","# These are temporary files if we need to download it from the original source (slow)\n","data_cache = './data/cache'\n","glove_full_tar = 'glove.6B.zip'\n","glove_full_50d = 'glove.6B.50d.txt'\n","\n","#force_download_from_original=False\n","download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n","original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n","\n","if not os.path.isfile( glove_100k_50d_path ):\n","    if not os.path.exists(glove_dir):\n","        os.makedirs(glove_dir)\n","    \n","    # First, try to download a pre-prepared file directly...\n","    response = requests.get(download_url, stream=True)\n","    if response.status_code == requests.codes.ok:\n","        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n","        with open(glove_100k_50d_path, 'wb') as out_file:\n","            shutil.copyfileobj(response.raw, out_file)\n","    else:\n","        # But, for some reason, RedCatLabs didn't give us the file directly\n","        if not os.path.exists(data_cache):\n","            os.makedirs(data_cache)\n","        \n","        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n","            zipfilepath = os.path.join(data_cache, glove_full_tar)\n","            if not os.path.isfile( zipfilepath ):\n","                print(\"Downloading 860Mb GloVE file from Stanford\")\n","                response = requests.get(download_url, stream=True)\n","                with open(zipfilepath, 'wb') as out_file:\n","                    shutil.copyfileobj(response.raw, out_file)\n","            if os.path.isfile(zipfilepath):\n","                print(\"Unpacking 50d GloVE file from zip\")\n","                import zipfile\n","                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n","\n","        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n","            with open(glove_100k_50d_path, 'wt') as out_file:\n","                print(\"Reducing 50d GloVE file to first 100k words\")\n","                for i, l in enumerate(in_file.readlines()):\n","                    if i>=100000: break\n","                    out_file.write(l)\n","    \n","        # Get rid of tarfile source (the required text file itself will remain)\n","        #os.unlink(zipfilepath)\n","        #os.unlink(os.path.join(data_cache, glove_full_50d))\n","\n","print(\"GloVE available locally\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading 42Mb pre-prepared GloVE file from RedCatLabs\n","GloVE available locally\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iz0T4wadKw4G","colab_type":"code","outputId":"92f2709d-69d1-41ac-83fe-2858511a8d47","executionInfo":{"status":"ok","timestamp":1587176601633,"user_tz":240,"elapsed":121035,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n","word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n","word_embedding.word_vectors.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100000, 50)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"ypvwew8AKw4I","colab_type":"text"},"source":["Having loaded that, play around with the similarity and analogy tests again..."]},{"cell_type":"code","metadata":{"id":"U4tTX53UKw4I","colab_type":"code","outputId":"6ab45585-acbf-475b-86f7-1c5b461cfe7d","executionInfo":{"status":"ok","timestamp":1587176601638,"user_tz":240,"elapsed":121036,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["word_embedding.most_similar('king')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('prince', 0.8236179693335699),\n"," ('queen', 0.7839043010964116),\n"," ('ii', 0.7746230030635107),\n"," ('emperor', 0.7736247624872925)]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"XvepxGvuKw4N","colab_type":"code","outputId":"4a96e31d-b079-4239-e9c6-dbcf8e8611d2","executionInfo":{"status":"ok","timestamp":1587176601738,"user_tz":240,"elapsed":121131,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["test_analogy('man woman king queen')\n","test_analogy('paris france rome italy')\n","test_analogy('kitten cat puppy dog')\n","test_analogy('understand understood run ran')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'man' is to 'woman' as 'king' is to {queen, daughter, prince, throne}\n","'paris' is to 'france' as 'rome' is to {italy, spain, portugal}\n","'kitten' is to 'cat' as 'puppy' is to {dog, rabbit, horse}\n","'understand' is to 'understood' as 'run' is to {ran, running, runs, twice}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AzqE6j7DKw4Q","colab_type":"text"},"source":["### Visualize Embedding in TensorBoard"]},{"cell_type":"code","metadata":{"id":"OEjeYj4AKw4R","colab_type":"code","outputId":"19e081f3-d188-4522-8690-03c2f33f13e5","executionInfo":{"status":"error","timestamp":1587176603763,"user_tz":240,"elapsed":123144,"user":{"displayName":"Kareem Husseini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiaWJZu8U0ugtLwOa_YN6qfx2RUdYW3T5AmEZNkvA=s64","userId":"03699348306200967420"}},"colab":{"base_uri":"https://localhost:8080/","height":381}},"source":["import tensorflow as tf\n","from tensorflow.contrib.tensorboard.plugins import projector\n","\n","#N = 10000 # Number of items (vocab size).\n","#D = 200 # Dimensionality of the embedding.\n","#embedding_var = tf.Variable(tf.random_normal([N,D]), name='word_embedding')\n","\n","embedding_var = tf.Variable(word_embedding.word_vectors, dtype='float32', \n","                            name='word_embedding')\n","\n","# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n","projector_config = projector.ProjectorConfig()\n","\n","# You can add multiple embeddings. Here we add only one.\n","embedding = projector_config.embeddings.add()\n","embedding.tensor_name = embedding_var.name\n","\n","# Link this tensor to its metadata file (e.g. labels).\n","LOG_DIR='../../tensorflow.logdir/'\n","os.makedirs(LOG_DIR, exist_ok=True)    \n","\n","metadata_file = 'glove_full_50d.words.tsv'\n","vocab_list = [ word_embedding.inverse_dictionary[i] \n","               for i in range(len( word_embedding.inverse_dictionary )) ]\n","with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:\n","    metadata.writelines(\"%s\\n\" % w for w in vocab_list)\n","    \n","embedding.metadata_path = os.path.join(os.getcwd(), LOG_DIR, metadata_file)\n","\n","# Use the same LOG_DIR where you stored your checkpoint.\n","summary_writer = tf.summary.FileWriter(LOG_DIR)\n","\n","# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n","# read this file during startup.\n","projector.visualize_embeddings(summary_writer, projector_config)\n","\n","saver = tf.train.Saver([embedding_var])\n","\n","with tf.Session() as sess:\n","    # Initialize the model\n","    sess.run(tf.global_variables_initializer())\n","    \n","    saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))\n","#print(\"Look at the embedding in TensorBoard : http://localhost:8081/\")"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-4c44a5f45b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#N = 10000 # Number of items (vocab size).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#D = 200 # Dimensionality of the embedding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"EzEJ__5vKw4T","colab_type":"text"},"source":["### Run TensorBoard via Colab "]},{"cell_type":"code","metadata":{"id":"yG3T8nfpKw4T","colab_type":"code","colab":{}},"source":["# Start the tensorboard server on this (colab) machine\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 8081 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kvJLu4TKw4V","colab_type":"code","colab":{}},"source":["# Install 'localtunnel' (a node.js proxy) -- work a little harder to avoid global install\n","! npm install localtunnel\n","\n","! ls -l node_modules/localtunnel/bin/client"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"brt_wGqzKw4X","colab_type":"code","colab":{}},"source":["# Tunnel port 8081 (TensorBoard assumed running)\n","get_ipython().system_raw('node_modules/localtunnel/bin/client --port 8081 >> tunnel_url.txt 2>&1 &')\n","\n","# Check that it's running\n","! ps fax | grep node | grep 8081"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1yGV_WIKw4Z","colab_type":"code","colab":{}},"source":["# Get url - this should be available on the web \n","#   (tunnels into colab via localtunnel to its tensorboard)\n","! cat tunnel_url.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44CzIZiwKw4d","colab_type":"text"},"source":["### Exercises\n","\n","1.  Plot some of the embeddings on a graph (potentially apply PCA first)\n","\n","    +  Nice example \"medical\"\n","\n","2.  ...\n","\n"]},{"cell_type":"code","metadata":{"id":"3HNDarWhKw4d","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}